{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9acb5-99c0-4b7f-8768-f298649b7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import scipy.stats as stats\n",
    "from scipy import stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443966e-15ab-43e8-b1c4-cb39257f2b87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(r'/Users/eliskasimova/Desktop/data_analytics_course_2024/project_folder/labs/second_project/data/clean/df_merged.csv')\n",
    "df_2 = pd.read_csv(r'/Users/eliskasimova/Desktop/data_analytics_course_2024/project_folder/labs/second_project/data/raw/df_final_web_data_pt_1.txt')\n",
    "df_3 = pd.read_csv(r'/Users/eliskasimova/Desktop/data_analytics_course_2024/project_folder/labs/second_project/data/raw/df_final_web_data_pt_2.txt')\n",
    "merged_df = pd.concat([df_2, df_3], axis=0)\n",
    "\n",
    "df_merged = df_1.merge(merged_df, on='client_id', how='inner')\n",
    "df_merged['date_time'] = pd.to_datetime(df_merged['date_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6462d6-51fd-4b80-ac58-9d51704622de",
   "metadata": {},
   "source": [
    "### Groups sorting to control/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b667030-dcbf-4ef8-a6fa-da2f4e2fb1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group = df_merged[df_merged['variation'] == 'Control']\n",
    "test_group = df_merged[df_merged['variation'] == 'Test']\n",
    "\n",
    "# Sort control group\n",
    "control_group_sorted = control_group.sort_values(by=['client_id', 'visit_id', 'process_step', 'date_time'])\n",
    "\n",
    "# Sort test group\n",
    "test_group_sorted = test_group.sort_values(by=['client_id', 'visit_id', 'process_step', 'date_time'])\n",
    "\n",
    "print(control_group_sorted.shape)\n",
    "print(test_group_sorted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7988e9-e479-47f5-b568-5bd9eb94adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define age bins and categorize ages\n",
    "bins = [0, 30, 40, 50, 100] \n",
    "labels = ['Under 30', '30-39', '40-49', '50 and above']\n",
    "df_merged['age_group'] = pd.cut(df_merged['clnt_age'], bins=bins, labels=labels)\n",
    "control_group_sorted['age_group'] = pd.cut(control_group_sorted['clnt_age'], bins=bins, labels=labels)\n",
    "test_group_sorted['age_group'] = pd.cut(test_group_sorted['clnt_age'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b0848-1a58-41f8-9584-e1099549888b",
   "metadata": {},
   "source": [
    "### Calculate Error Rates\n",
    "The error rates are calculated using the formula:  \n",
    "- **Control Group Error Rate** = (Clients with errors in control) / (Total clients in control)  \n",
    "- **Test Group Error Rate** = (Clients with errors in test) / (Total clients in test)\n",
    "- an analysis ensuring that the observed increase in completion rate from the A/B test meets or exceeds this 5% threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f73dc0f-8b60-4e24-939f-f95faba05bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error rate calculation\n",
    "\n",
    "def calculate_errors(group):\n",
    "    # Check if current step is before the last completed step\n",
    "    group['step_index'] = group['process_step'].map({'start': 0, 'step_1': 1, 'step_2': 2, 'step_3': 3, 'confirm': 4})\n",
    "    group['error'] = group['step_index'].diff().apply(lambda x: x < 0)  # Negative diff indicates a backward step\n",
    "    return group\n",
    "\n",
    "# Apply error calculation\n",
    "control_group_sorted = calculate_errors(control_group_sorted)\n",
    "test_group_sorted = calculate_errors(test_group_sorted)\n",
    "\n",
    "# Calculate Error Rates\n",
    "control_error_rate = control_group_sorted['error'].mean() * 100\n",
    "test_error_rate = test_group_sorted['error'].mean() * 100\n",
    "\n",
    "print(f\"Control Group Error Rate: {control_error_rate:.2f}%\")\n",
    "print(f\"Test Group Error Rate: {test_error_rate:.2f}%\")\n",
    "\n",
    "#Check if the test group's error rate is smaller than the control group by at least 5%\n",
    "error_rate_difference = control_error_rate - test_error_rate  # Difference between control and test error rates\n",
    "threshold = 5  # 5% threshold for the difference\n",
    "\n",
    "# Perform the analysis\n",
    "if error_rate_difference >= threshold:\n",
    "    print(f\"The test group has an error rate that is at least {threshold}% smaller than the control group.\")\n",
    "else:\n",
    "    print(f\"The test group does not have an error rate that is at least {threshold}% smaller than the control group.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec011e3-0862-4c86-81de-3ba6bbb8d3c1",
   "metadata": {},
   "source": [
    "### Erorr rates by age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17291a0f-fcb6-44d1-b974-d225f5e7bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_error_by_age(group, age_column='age_group'):\n",
    "    # Group data by age group and calculate the average error rate\n",
    "    error_summary = (\n",
    "        group.groupby(age_column)['error']\n",
    "        .mean()\n",
    "        .reset_index(name='average_error_rate')  # Calculate mean error rate\n",
    "    )\n",
    "    # Sort the results by age group\n",
    "    error_summary['average_error_rate'] *= 100 # get the percentage\n",
    "    error_summary = error_summary.sort_values(by=age_column)\n",
    "    return error_summary\n",
    "\n",
    "# Calculate for control and test groups\n",
    "control_error_by_age = calculate_average_error_by_age(control_group_sorted)\n",
    "test_error_by_age = calculate_average_error_by_age(test_group_sorted)\n",
    "\n",
    "control_error_by_age['average_error_rate'] = control_error_by_age['average_error_rate'].round(2)\n",
    "test_error_by_age['average_error_rate'] = test_error_by_age['average_error_rate'].round(2)\n",
    "\n",
    "\n",
    "# Display the results\n",
    "print(\"Control Group Error Rates by Age and Tenure: (%)\")\n",
    "print(control_error_by_age)\n",
    "\n",
    "print(\"Test Group Error Rates by Age and Tenure: (%)\")\n",
    "print(test_error_by_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bb52b-0d33-4aa5-a7f1-9de3e0e8cf52",
   "metadata": {},
   "source": [
    "### Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707429e5-db23-4a10-b1c6-173687253e45",
   "metadata": {},
   "source": [
    "### Binominal hypothesis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d961c7d5-aee6-48d4-b6c4-50e5be630f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binomtest  #import the binomial test function\n",
    "\n",
    "# calculate the total errors and total steps for each group\n",
    "control_errors = control_group_sorted['error'].sum()  # total errors in the control group\n",
    "control_total = len(control_group_sorted)  # total steps (rows) in the control group\n",
    "test_errors = test_group_sorted['error'].sum()  # total errors in the test group\n",
    "test_total = len(test_group_sorted)  # total steps (rows) in the test group\n",
    "\n",
    "# calculate the error rates for each group\n",
    "control_error_rate = (control_errors / control_total) * 100  # Error rate as a percentage for control group\n",
    "test_error_rate = (test_errors / test_total) * 100  # Error rate as a percentage for test group\n",
    "\n",
    "# calculate the percentage difference in error rates between the two groups\n",
    "percentage_difference = control_error_rate - test_error_rate  # Difference in error rates (control - test)\n",
    "\n",
    "# perform a one-tailed binomial test\n",
    "# H0 =  Test group error rate >= Control group error rate\n",
    "# H1 = Test group error rate < Control group error rate\n",
    "result = binomtest(test_errors, test_total, control_errors / control_total, alternative='less')\n",
    "\n",
    "# extract the p-value from the test result\n",
    "p_value = result.pvalue \n",
    "\n",
    "# display the calculated error rates, percentage difference, and p-value\n",
    "print(f\"Control group error rate: {control_error_rate:.2f}%\")  # display control group error rate\n",
    "print(f\"Test group error rate: {test_error_rate:.2f}%\")  # display test group error rate\n",
    "print(f\"Percentage difference: {percentage_difference:.2f}%\")  # display difference in error rates\n",
    "print(f\"P-value: {p_value:.4f}\")  # display the p-value for the binomial test\n",
    "\n",
    "# interpret the results of the binomial test\n",
    "alpha = 0.05  # Set the significance level (threshold for p-value)\n",
    "if p_value < alpha:\n",
    "    # if p-value is less than alpha, reject the null hypothesis\n",
    "    print(\"The test group has significantly lower error rate than the control group.\")\n",
    "else:\n",
    "    # if p-value is greater than or equal to alpha, fail to reject the null hypothesis\n",
    "    print(\"The test group does not have a significantlly lower error rate than the control group.\")\n",
    "\n",
    "# interpret the practical difference between error rates\n",
    "if percentage_difference >= 5:\n",
    "    # if the percentage difference meets or exceeds the 5% threshold\n",
    "    print(\"The test group's error rate is at least 5% lower than the control group.\")\n",
    "else:\n",
    "    # if the percentage difference is less than the 5% threshold\n",
    "    print(\"The test group's error rate is not at least 5% lower than the control group.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63bb68-4604-4fa3-9526-14a6146e1ba4",
   "metadata": {},
   "source": [
    "### Two Sample T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7874d2de-200f-4227-b8ce-8722f6465070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting boolean values to integers (1 for True, 0 for False)\n",
    "control_error_rate = control_error_rate.astype(int)\n",
    "test_error_rate = test_error_rate.astype(int)\n",
    "\n",
    "# checking there is data after dropping NaN values\n",
    "print(f\"Control Error Rate Length: {len(control_error_rate)}\")\n",
    "print(f\"Test Error Rate Length: {len(test_error_rate)}\")\n",
    "\n",
    "# performing the test\n",
    "try:\n",
    "    stat, p_value = stats.ttest_ind(control_error_rate, test_error_rate, equal_var=False, alternative='two-sided')\n",
    "    print(f\"T-statistic: {stat:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "    # settting the significance level\n",
    "    alpha = 0.05 \n",
    "\n",
    "    if p_value < alpha:\n",
    "        print(\"Reject the null hypothesis: There is a significant difference in error rates between the control and test groups.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: There is no significant difference in error rates between the control and test groups.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ee987-4850-40d6-8ebd-867b2c94ea68",
   "metadata": {},
   "source": [
    "### Conclusion and Comparison:\n",
    "\n",
    "Statistical Significance: Both the binomial test and the two-sample t-test show that there is a statistically significant difference between the error rates of the control and test groups (p-value = 0.0000 in both cases).\n",
    "\n",
    "Practical Significance: The percentage difference in error rates between the two groups is 1.57%, which is not above your 5% threshold. This means the test group is better (in terms of error rate), but the improvement is small, and it may not be large enough to justify significant action.\n",
    "\n",
    "The tests show a statistically significant difference in error rates between the groups. However, the 1.57% improvement in the test group may not be large enough to justify changes, especially if we expect at least a 5% improvement. If even a small reduction in errors is valuable, the result can still be useful, otherwise it might not be worth acting on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5149fc-25b5-4d2e-a802-73ca4c5b317b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

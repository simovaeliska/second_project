{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac9acb5-99c0-4b7f-8768-f298649b7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import scipy.stats as st\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e443966e-15ab-43e8-b1c4-cb39257f2b87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(r'/Users/eliskasimova/Desktop/data_analytics_course_2024/project_folder/labs/second_project/data/clean/df_merged.csv')\n",
    "df_2 = pd.read_csv(r'/Users/eliskasimova/Desktop/data_analytics_course_2024/project_folder/labs/second_project/data/raw/df_final_web_data_pt_1.txt')\n",
    "df_3 = pd.read_csv(r'/Users/eliskasimova/Desktop/data_analytics_course_2024/project_folder/labs/second_project/data/raw/df_final_web_data_pt_2.txt')\n",
    "merged_df = pd.concat([df_2, df_3], axis=0)\n",
    "\n",
    "df_merged = df_1.merge(merged_df, on='client_id', how='inner')\n",
    "df_merged['date_time'] = pd.to_datetime(df_merged['date_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6462d6-51fd-4b80-ac58-9d51704622de",
   "metadata": {},
   "source": [
    "### Groups sorting to control/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b667030-dcbf-4ef8-a6fa-da2f4e2fb1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143408, 14)\n",
      "(177779, 14)\n"
     ]
    }
   ],
   "source": [
    "control_group = df_merged[df_merged['variation'] == 'Control']\n",
    "test_group = df_merged[df_merged['variation'] == 'Test']\n",
    "\n",
    "# Sort control group\n",
    "control_group_sorted = control_group.sort_values(by=['client_id', 'visit_id', 'process_step', 'date_time'])\n",
    "\n",
    "# Sort test group\n",
    "test_group_sorted = test_group.sort_values(by=['client_id', 'visit_id', 'process_step', 'date_time'])\n",
    "\n",
    "print(control_group_sorted.shape)\n",
    "print(test_group_sorted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d7988e9-e479-47f5-b568-5bd9eb94adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define age bins and categorize ages\n",
    "bins = [0, 30, 40, 50, 100] \n",
    "labels = ['Under 30', '30-39', '40-49', '50 and above']\n",
    "df_merged['age_group'] = pd.cut(df_merged['clnt_age'], bins=bins, labels=labels)\n",
    "control_group_sorted['age_group'] = pd.cut(control_group_sorted['clnt_age'], bins=bins, labels=labels)\n",
    "test_group_sorted['age_group'] = pd.cut(test_group_sorted['clnt_age'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b0848-1a58-41f8-9584-e1099549888b",
   "metadata": {},
   "source": [
    "### Calculate Error Rates\n",
    "The error rates are calculated using the formula:  \n",
    "- **Control Group Error Rate** = (Clients with errors in control) / (Total clients in control)  \n",
    "- **Test Group Error Rate** = (Clients with errors in test) / (Total clients in test)\n",
    "- an analysis ensuring that the observed increase in completion rate from the A/B test meets or exceeds this 5% threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f73dc0f-8b60-4e24-939f-f95faba05bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error rate calculation function\n",
    "def calculate_errors(group):\n",
    "    # Map process steps to step indices\n",
    "    group['step_index'] = group['process_step'].map({'start': 0, 'step_1': 1, 'step_2': 2, 'step_3': 3, 'confirm': 4})\n",
    "    \n",
    "    # Calculate error: Negative diff indicates backward movement\n",
    "    group['error'] = group['step_index'].diff().apply(lambda x: x < 0)\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Apply the error calculation to both control and test groups\n",
    "control_group_sorted = calculate_errors(control_group_sorted)\n",
    "test_group_sorted = calculate_errors(test_group_sorted)\n",
    "\n",
    "# Calculate the overall error rates (as percentages)\n",
    "control_error_rate = control_group_sorted['error'].mean() * 100\n",
    "test_error_rate = test_group_sorted['error'].mean() * 100\n",
    "\n",
    "# Print the overall error rates\n",
    "print(f\"Control Group Error Rate: {control_error_rate:.2f}%\")\n",
    "print(f\"Test Group Error Rate: {test_error_rate:.2f}%\")\n",
    "\n",
    "# Calculate the difference in error rates between control and test group\n",
    "error_rate_difference = control_error_rate - test_error_rate  # Control error rate minus Test error rate\n",
    "\n",
    "# Set the threshold for a 5% difference\n",
    "threshold = 5\n",
    "\n",
    "# Check if the test group error rate is at least 5% smaller than the control group\n",
    "if error_rate_difference >= threshold:\n",
    "    print(f\"The test group has an error rate that is at least {threshold}% smaller than the control group.\")\n",
    "else:\n",
    "    print(f\"The test group does not have an error rate that is at least {threshold}% smaller than the control group.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c83cca-5c32-4850-a1a9-7761445208aa",
   "metadata": {},
   "source": [
    "### Calculating errors per each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e5149fc-25b5-4d2e-a802-73ca4c5b317b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control Group Error Rate: 19.21%\n",
      "Test Group Error Rate: 17.64%\n",
      "\n",
      "Control Group Error Rates Per Step:\n",
      "process_step\n",
      "confirm     0.000000\n",
      "start      56.941730\n",
      "step_1      0.792393\n",
      "step_2      0.332341\n",
      "step_3      0.574889\n",
      "Name: error, dtype: float64\n",
      "\n",
      "Test Group Error Rates Per Step:\n",
      "process_step\n",
      "confirm     0.000000\n",
      "start      54.635322\n",
      "step_1      0.588387\n",
      "step_2      0.342178\n",
      "step_3      0.753245\n",
      "Name: error, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Mapping process steps to step indices\n",
    "step_index_map = {'start': 0, 'step_1': 1, 'step_2': 2, 'step_3': 3, 'confirm': 4}\n",
    "\n",
    "# Function to calculate backward steps (errors)\n",
    "def calculate_errors(group):\n",
    "    # Map process_step to step_index\n",
    "    group['step_index'] = group['process_step'].map(step_index_map)\n",
    "    \n",
    "    # Calculate the difference between consecutive step indices\n",
    "    group['error'] = group['step_index'].diff().apply(lambda x: x < 0)  # Negative diff indicates backward step\n",
    "    \n",
    "    # Fill NaN values for the first row in each group (no previous step to compare)\n",
    "    group['error'] = group['error'].fillna(False)\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Apply the error calculation to the control and test groups\n",
    "control_group_sorted = calculate_errors(control_group_sorted)\n",
    "test_group_sorted = calculate_errors(test_group_sorted)\n",
    "\n",
    "# Calculate the overall error rates for control and test groups\n",
    "control_error_rate = control_group_sorted['error'].mean() * 100  # Multiply by 100 to get percentage\n",
    "test_error_rate = test_group_sorted['error'].mean() * 100\n",
    "\n",
    "# Print overall error rates\n",
    "print(f\"Control Group Error Rate: {control_error_rate:.2f}%\")\n",
    "print(f\"Test Group Error Rate: {test_error_rate:.2f}%\")\n",
    "\n",
    "# Calculate error rates per step (process step by process step)\n",
    "def error_rate_per_step(group):\n",
    "    return group.groupby('process_step')['error'].mean() * 100  # Multiply by 100 to get percentage\n",
    "\n",
    "# Calculate error rates per step for both groups\n",
    "control_error_rates_per_step = error_rate_per_step(control_group_sorted)\n",
    "test_error_rates_per_step = error_rate_per_step(test_group_sorted)\n",
    "\n",
    "# Display the error rates per step for both groups\n",
    "print(\"\\nControl Group Error Rates Per Step:\")\n",
    "print(control_error_rates_per_step)\n",
    "\n",
    "print(\"\\nTest Group Error Rates Per Step:\")\n",
    "print(test_error_rates_per_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec011e3-0862-4c86-81de-3ba6bbb8d3c1",
   "metadata": {},
   "source": [
    "### Erorr rates by age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17291a0f-fcb6-44d1-b974-d225f5e7bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_error_by_age(group, age_column='age_group'):\n",
    "    # Group data by age group and calculate the average error rate\n",
    "    error_summary = (\n",
    "        group.groupby(age_column)['error']\n",
    "        .mean()\n",
    "        .reset_index(name='average_error_rate')  # Calculate mean error rate\n",
    "    )\n",
    "    # Sort the results by age group\n",
    "    error_summary['average_error_rate'] *= 100 # get the percentage\n",
    "    error_summary = error_summary.sort_values(by=age_column)\n",
    "    return error_summary\n",
    "\n",
    "# Calculate for control and test groups\n",
    "control_error_by_age = calculate_average_error_by_age(control_group_sorted)\n",
    "test_error_by_age = calculate_average_error_by_age(test_group_sorted)\n",
    "\n",
    "control_error_by_age['average_error_rate'] = control_error_by_age['average_error_rate'].round(2)\n",
    "test_error_by_age['average_error_rate'] = test_error_by_age['average_error_rate'].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bb52b-0d33-4aa5-a7f1-9de3e0e8cf52",
   "metadata": {},
   "source": [
    "### Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707429e5-db23-4a10-b1c6-173687253e45",
   "metadata": {},
   "source": [
    "### Binominal hypothesis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d961c7d5-aee6-48d4-b6c4-50e5be630f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control group error rate: 19.21%\n",
      "Test group error rate: 17.64%\n",
      "Percentage difference: 1.57%\n",
      "P-value: 0.0000\n",
      "The test group has significantly lower error rate than the control group.\n",
      "The test group's error rate is not at least 5% lower than the control group.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import binomtest  #import the binomial test function\n",
    "\n",
    "# calculate the total errors and total steps for each group\n",
    "control_errors = control_group_sorted['error'].sum()  # total errors in the control group\n",
    "control_total = len(control_group_sorted)  # total steps (rows) in the control group\n",
    "test_errors = test_group_sorted['error'].sum()  # total errors in the test group\n",
    "test_total = len(test_group_sorted)  # total steps (rows) in the test group\n",
    "\n",
    "# calculate the error rates for each group\n",
    "control_error_rate = (control_errors / control_total) * 100  # Error rate as a percentage for control group\n",
    "test_error_rate = (test_errors / test_total) * 100  # Error rate as a percentage for test group\n",
    "\n",
    "# calculate the percentage difference in error rates between the two groups\n",
    "percentage_difference = control_error_rate - test_error_rate  # Difference in error rates (control - test)\n",
    "\n",
    "# perform a one-tailed binomial test\n",
    "# H0 =  Test group error rate >= Control group error rate\n",
    "# H1 = Test group error rate < Control group error rate\n",
    "result = binomtest(test_errors, test_total, control_errors / control_total, alternative='less')\n",
    "\n",
    "# extract the p-value from the test result\n",
    "p_value = result.pvalue \n",
    "\n",
    "# display the calculated error rates, percentage difference, and p-value\n",
    "print(f\"Control group error rate: {control_error_rate:.2f}%\")  # display control group error rate\n",
    "print(f\"Test group error rate: {test_error_rate:.2f}%\")  # display test group error rate\n",
    "print(f\"Percentage difference: {percentage_difference:.2f}%\")  # display difference in error rates\n",
    "print(f\"P-value: {p_value:.4f}\")  # display the p-value for the binomial test\n",
    "\n",
    "# interpret the results of the binomial test\n",
    "alpha = 0.05  # Set the significance level (threshold for p-value)\n",
    "if p_value < alpha:\n",
    "    # if p-value is less than alpha, reject the null hypothesis\n",
    "    print(\"The test group has significantly lower error rate than the control group.\")\n",
    "else:\n",
    "    # if p-value is greater than or equal to alpha, fail to reject the null hypothesis\n",
    "    print(\"The test group does not have a significantlly lower error rate than the control group.\")\n",
    "\n",
    "# interpret the practical difference between error rates\n",
    "if percentage_difference >= 5:\n",
    "    # if the percentage difference meets or exceeds the 5% threshold\n",
    "    print(\"The test group's error rate is at least 5% lower than the control group.\")\n",
    "else:\n",
    "    # if the percentage difference is less than the 5% threshold\n",
    "    print(\"The test group's error rate is not at least 5% lower than the control group.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63bb68-4604-4fa3-9526-14a6146e1ba4",
   "metadata": {},
   "source": [
    "### Two Sample T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7874d2de-200f-4227-b8ce-8722f6465070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control Error Rate Length: 143408\n",
      "Test Error Rate Length: 177779\n",
      "Statistic: 11.3619\n",
      "P-value: 0.0000\n",
      "Reject the null hypothesis: There is a significant difference in error rates between the control and test groups.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#identifying error columns\n",
    "control_error_rate = control_group_sorted['error']\n",
    "test_error_rate = test_group_sorted['error']\n",
    "\n",
    "# converting boolean values to integers (1 for True, 0 for False)\n",
    "control_error_rate = control_error_rate.astype(int)\n",
    "test_error_rate = test_error_rate.astype(int)\n",
    "\n",
    "# droping nan values\n",
    "control_error_rate = control_error_rate.dropna()\n",
    "test_error_rate = test_error_rate.dropna()\n",
    "\n",
    "# check if there is data after dropping NaN values\n",
    "print(f\"Control Error Rate Length: {len(control_error_rate)}\")\n",
    "print(f\"Test Error Rate Length: {len(test_error_rate)}\")\n",
    "\n",
    "# Perform the independent t-test\n",
    "try:\n",
    "    _, p_value = st.ttest_ind(control_error_rate, test_error_rate, equal_var=False, alternative='two-sided')\n",
    "    print(f\"Statistic: {_:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "    # Set significance level\n",
    "    alpha = 0.05  # Significance level\n",
    "\n",
    "    # Decision rule\n",
    "    if p_value < alpha:\n",
    "        print(\"Reject the null hypothesis: There is a significant difference in error rates between the control and test groups.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: There is no significant difference in error rates between the control and test groups.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ee987-4850-40d6-8ebd-867b2c94ea68",
   "metadata": {},
   "source": [
    "### Conclusion and Comparison:\n",
    "\n",
    "Statistical Significance: Both the binomial test and the two-sample t-test show that there is a statistically significant difference between the error rates of the control and test groups (p-value = 0.0000 in both cases).\n",
    "\n",
    "Practical Significance: The percentage difference in error rates between the two groups is 1.57%, which is not above 5% threshold. This means the test group is better (in terms of error rate), but the improvement is small, and it may not be large enough to justify significant action.\n",
    "\n",
    "The tests show a statistically significant difference in error rates between the groups. However, the 1.57% improvement in the test group may not be large enough to justify changes, especially if we expect at least a 5% improvement. If even a small reduction in errors is valuable, the result can still be useful, otherwise it might not be worth acting on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd65b4-e93c-49fa-a535-96bcfa7595db",
   "metadata": {},
   "source": [
    "### Creating data frame to convert to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5219fa93-5197-4736-9958-8263bf162973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame combining the error rates for both groups\n",
    "error_rates_df = pd.DataFrame({\n",
    "    'process_step': control_error_rates_per_step.index,\n",
    "    'control_group_error_rate (%)': control_error_rates_per_step.values,\n",
    "    'test_group_error_rate (%)': test_error_rates_per_step.values\n",
    "})\n",
    "\n",
    "# Round the error rates to 2 decimal places\n",
    "error_rates_df['control_group_error_rate (%)'] = error_rates_df['control_group_error_rate (%)'].round(2)\n",
    "error_rates_df['test_group_error_rate (%)'] = error_rates_df['test_group_error_rate (%)'].round(2)\n",
    "\n",
    "# Display the DataFrame for inspection\n",
    "print(error_rates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "701d597e-c8b2-4908-924d-5c411ca0b609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control Group Error Rate: 19.21%\n",
      "Test Group Error Rate: 17.64%\n",
      "The test group does not have an error rate that is at least 5% smaller than the control group.\n"
     ]
    }
   ],
   "source": [
    "# Error rate calculation function\n",
    "def calculate_errors(group):\n",
    "    # Map process steps to step indices\n",
    "    group['step_index'] = group['process_step'].map({'start': 0, 'step_1': 1, 'step_2': 2, 'step_3': 3, 'confirm': 4})\n",
    "    \n",
    "    # Calculate error: Negative diff indicates backward movement\n",
    "    group['error'] = group['step_index'].diff().apply(lambda x: x < 0)\n",
    "    \n",
    "    # Calculate total errors and total steps\n",
    "    total_errors = group['error'].sum()\n",
    "    total_steps = len(group)\n",
    "    \n",
    "    return total_errors, total_steps\n",
    "\n",
    "# Calculate errors and steps for both control and test groups\n",
    "control_errors, control_steps = calculate_errors(control_group_sorted)\n",
    "test_errors, test_steps = calculate_errors(test_group_sorted)\n",
    "\n",
    "# Calculate error rates\n",
    "control_error_rate = (control_errors / control_steps) * 100\n",
    "test_error_rate = (test_errors / test_steps) * 100\n",
    "\n",
    "# Print the overall error rates\n",
    "print(f\"Control Group Error Rate: {control_error_rate:.2f}%\")\n",
    "print(f\"Test Group Error Rate: {test_error_rate:.2f}%\")\n",
    "\n",
    "# Calculate the difference in error rates\n",
    "error_rate_difference = control_error_rate - test_error_rate\n",
    "\n",
    "# Set the threshold for a 5% difference\n",
    "threshold = 5\n",
    "\n",
    "# Hypothesis test: Is the test group's error rate at least 5% smaller?\n",
    "if error_rate_difference >= threshold:\n",
    "    print(f\"The test group has an error rate that is at least {threshold}% smaller than the control group.\")\n",
    "else:\n",
    "    print(f\"The test group does not have an error rate that is at least {threshold}% smaller than the control group.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
